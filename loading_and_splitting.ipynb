{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a740368-16b9-4c29-b5c5-9ef43a94d422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yvettegonzalez/miniforge3/lib/python3.9/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.25) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77318bb-cbbc-4f8a-8b66-208877867ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('sample_filings', show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38702472-6d82-4657-b8f3-08eb473f48bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████▎                                 | 1/4 [01:05<03:17, 65.92s/it]\n"
     ]
    }
   ],
   "source": [
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91873aa0-6761-477c-a571-905d3eecf89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ecd292-1f19-4c3a-9e2a-b6a26b6ceac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c65183a-b011-4086-b9d5-fb8312594640",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89f0e537-ef44-43a6-aa98-1327af857b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.text_splitter import TextSplitter\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5adfde0a-2c4f-4b1f-b165-2737e872088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextSplitter(TextSplitter):\n",
    "\n",
    "    def split_text(self, document):\n",
    "        text = document.page_content \n",
    "        \n",
    "        # Extract metadata\n",
    "        filed_as_of_date_match = re.search(r'FILED AS OF DATE:\\s*(\\d+)', text)\n",
    "        filed_as_of_date = filed_as_of_date_match.group(1) if filed_as_of_date_match else None\n",
    "        \n",
    "        company_name_match = re.search(r'COMPANY CONFORMED NAME:\\s*([^\\n]+)', text)\n",
    "        company_name = company_name_match.group(1).strip() if company_name_match else None\n",
    "\n",
    "        sic_match = re.search(r'STANDARD INDUSTRIAL CLASSIFICATION:\\s*([^\\n]+)', text)\n",
    "        sic = sic_match.group(1).strip() if sic_match else None\n",
    "\n",
    "        state_match = re.search(r'STATE:\\s*([^\\n]+)', text)\n",
    "        state = state_match.group(1).strip() if state_match else None\n",
    "        \n",
    "        # Regex to find <DOCUMENT> tags\n",
    "        doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "        doc_end_pattern = re.compile(r'</DOCUMENT>')\n",
    "        type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
    "\n",
    "        \n",
    "        doc_start_is = [x.end() for x in doc_start_pattern.finditer(text)]\n",
    "        doc_end_is = [x.start() for x in doc_end_pattern.finditer(text)]\n",
    "        doc_types = [x[len('<TYPE>'):] for x in type_pattern.findall(text)]\n",
    "        \n",
    "        \n",
    "        document = {}\n",
    "\n",
    "        # Create a loop to go through each section type and save only the 10-K section in the dictionary\n",
    "        for doc_type, doc_start, doc_end in zip(doc_types, doc_start_is, doc_end_is):\n",
    "            if doc_type == '10-K':\n",
    "                document[doc_type] = text[doc_start:doc_end]\n",
    "                \n",
    "                \n",
    "        regex = re.compile(r'(>Item(\\s|&#160;|&nbsp;)(1A|1B|2|3|4|5|6|7|7A|8|9A|9B|9|[1][0-5])\\.{0,1})|(ITEM(\\s|&#160;|&nbsp;)(1A|1B|2|3|4|5|6|7|7A|8|9A|9B|9|[1][0-5])\\.{0,1})')\n",
    "        matches = regex.finditer(document['10-K'])\n",
    "        # Create the dataframe\n",
    "        test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])\n",
    "\n",
    "        test_df.columns = ['item', 'start', 'end']\n",
    "        test_df['item'] = test_df.item.str.lower()\n",
    "        \n",
    "        # Get rid of unnesesary charcters from the dataframe\n",
    "        test_df.replace('&#160;',' ',regex=True,inplace=True)\n",
    "        test_df.replace('&nbsp;',' ',regex=True,inplace=True)\n",
    "        test_df.replace(' ','',regex=True,inplace=True)\n",
    "        test_df.replace('\\.','',regex=True,inplace=True)\n",
    "        test_df.replace('>','',regex=True,inplace=True)\n",
    "        \n",
    "        # Drop duplicates\n",
    "        pos_dat = test_df.sort_values('start', ascending=True).drop_duplicates(subset=['item'], keep='last')\n",
    "        \n",
    "        sections = ['item1a','item1b','item2','item3','item4','item5','item6','item7',\n",
    "                 'item8','item9a','item9b','item9','item10','item11','item12','item13','item14',\n",
    "                 'item15']        \n",
    "        \n",
    "        cleaned_sections = []\n",
    "\n",
    "        for index, value in enumerate(sections):\n",
    "            if value == 'item15':\n",
    "                break\n",
    "            cur_start = pos_dat[pos_dat['item'] == value]['start'].values[0]\n",
    "            next_start = pos_dat[pos_dat['item'] == sections[index+1]]['start'].values[0]\n",
    "            \n",
    "            section = document['10-K'][cur_start:next_start]\n",
    "                        \n",
    "            # Clean up the date using BeautifulSoup\n",
    "            content = BeautifulSoup(section, \"lxml\")\n",
    "\n",
    "            # Append metadata with each section\n",
    "            metadata = {\n",
    "                \"company\": company_name,\n",
    "                \"date\": filed_as_of_date,\n",
    "                \"sic\": sic,\n",
    "                \"state\": state\n",
    "                \n",
    "            }\n",
    "\n",
    "            cleaned_section = {\n",
    "                \"content\": content.get_text(\"\\n\\n\"),\n",
    "                \"metadata\": metadata\n",
    "            }\n",
    "\n",
    "            cleaned_sections.append(cleaned_section)\n",
    "\n",
    "        return cleaned_sections\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b546090-84aa-4a50-b236-9dee93c2e183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1a42f39-8fac-4271-92ca-50f617fb0c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['item1a', 'item1b', 'item2', 'item3', 'item4', 'item5', 'item6', 'item7', 'item8', 'item9a', 'item9b', 'item9', 'item10', 'item11', 'item12', 'item13', 'item14', 'item15']\n"
     ]
    }
   ],
   "source": [
    "# Convert the Langchain Document to text\n",
    "# document_text = pages[0].page_content\n",
    "custom_splitter = CustomTextSplitter()\n",
    "\n",
    "# Split the document's text into sections, clean up the date, and append metadata\n",
    "sections = custom_splitter.split_text(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88bfc924-8ac5-400a-808b-549e45f4a083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '>Item 13.\\xa0\\xa0\\xa0\\xa0Certain Relationships and Related Transactions, and Director Independence\\n\\nThe information required by this Item will be included in the 2023 Proxy Statement, and is incorporated herein by reference.',\n",
       " 'metadata': {'company': 'Apple Inc.',\n",
       "  'date': '20221028',\n",
       "  'sic': 'ELECTRONIC COMPUTERS [3571]',\n",
       "  'state': 'CA'}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae959a0-c4b2-4558-8646-f1fb633181c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
