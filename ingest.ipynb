{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "279d7fc1-6e07-4de3-8631-cc761770e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "import re\n",
    "from langchain.text_splitter import TextSplitter\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5f7a3792-41e2-4b14-86dd-97167782f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1ec7249b-01c7-46b5-a5a7-3ad9b87ca4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf2be06-9ee2-46e0-b5ea-bb9bdb3408ab",
   "metadata": {},
   "source": [
    "## Load Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "63a68677-9899-4063-9604-247f94dc73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('sample_filings', show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "56413c88-99ed-4884-b848-9ad74173afde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▊                                   | 1/5 [02:16<09:05, 136.38s/it]\n"
     ]
    }
   ],
   "source": [
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9bf726b4-090d-4055-807f-8df3e95c1d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71e6a7-e135-44bf-834d-d2300a7f818f",
   "metadata": {},
   "source": [
    "## Custom Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f5f4457b-4f44-47a7-9470-c90f54ad2564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:48: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:48: DeprecationWarning: invalid escape sequence \\.\n",
      "/var/folders/68/8hrdhbcs24z7sr41_6qzmxkw0000gn/T/ipykernel_78243/3942246277.py:48: DeprecationWarning: invalid escape sequence \\.\n",
      "  df['item'] = df['item'].str.lower().str.replace('&#160;|&nbsp;| |\\.', '', regex=True).str.replace('>', '', regex=False)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class CustomTextSplitter(TextSplitter):\n",
    "\n",
    "    def split_text(self, document):\n",
    "        text = document.page_content \n",
    "\n",
    "        metadata = {\n",
    "            \"company\": self.extract_metadata(r'COMPANY CONFORMED NAME:\\s*([^\\n]+)', text),\n",
    "            \"date\": self.extract_metadata(r'FILED AS OF DATE:\\s*(\\d+)', text),\n",
    "            \"sic\": self.extract_metadata(r'STANDARD INDUSTRIAL CLASSIFICATION:\\s*([^\\n]+)', text),\n",
    "            \"state\": self.extract_metadata(r'STATE:\\s*([^\\n]+)', text)\n",
    "        }\n",
    "\n",
    "        ten_k_text = self.extract_10k(text)\n",
    "        \n",
    "        sections_df = self.get_sections_dataframe(ten_k_text)\n",
    "        # display(sections_df)\n",
    "\n",
    "        return self.extract_cleaned_sections(ten_k_text, sections_df, metadata)\n",
    "\n",
    "    def extract_metadata(self, pattern, text):\n",
    "        match = re.search(pattern, text)\n",
    "        return match.group(1).strip() if match else None\n",
    "\n",
    "    def extract_10k(self, text):\n",
    "        doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "        doc_end_pattern = re.compile(r'</DOCUMENT>')\n",
    "        type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
    "        \n",
    "        doc_start_is = [x.end() for x in doc_start_pattern.finditer(text)]\n",
    "        doc_end_is = [x.start() for x in doc_end_pattern.finditer(text)]\n",
    "        doc_types = [x[len('<TYPE>'):] for x in type_pattern.findall(text)]\n",
    "\n",
    "        for doc_type, doc_start, doc_end in zip(doc_types, doc_start_is, doc_end_is):\n",
    "            if doc_type == '10-K':\n",
    "                return text[doc_start:doc_end]\n",
    "\n",
    "        return None\n",
    "\n",
    "    def get_sections_dataframe(self, ten_k_text):\n",
    "        regex = re.compile(r'(>Item(\\s|&#160;|&nbsp;)(1.|1A|1B|2|3|4|5|6|7.|7A|8.|9A|9B|9.|[1][0-5])\\.{0,1})|(ITEM(\\s|&#160;|&nbsp;)(1.|1A|1B|2|3|4|5|6|7.|7A|8.|9A|9B|9.|[1][0-5])\\.{0,1})')\n",
    "        matches = regex.finditer(ten_k_text)\n",
    "\n",
    "        df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches], columns=['item', 'start', 'end'])\n",
    "        df['item'] = df['item'].str.lower().str.replace('&#160;|&nbsp;| |\\.', '', regex=True).str.replace('>', '', regex=False)\n",
    "\n",
    "        # Filter for the last occurrence of each section\n",
    "        df = df.groupby('item').last().reset_index()\n",
    "\n",
    "        # Map sections to a predefined order\n",
    "        section_order = {\n",
    "            'item1': 1, 'item1a': 2, 'item1b': 3, 'item2': 4, 'item3': 5, 'item4': 6, \n",
    "            'item5': 7, 'item6': 8, 'item7': 9, 'item7a': 10, 'item8': 11, 'item9': 12,\n",
    "            'item9a': 13, 'item9b': 14, 'item9c': 15, 'item10': 16, 'item11': 17, 'item12': 18, \n",
    "            'item13': 19, 'item14': 20, 'item15': 21, 'item16': 22\n",
    "        }\n",
    "        df['order'] = df['item'].map(section_order)\n",
    "\n",
    "        # Sort by predefined order\n",
    "        return df.sort_values(by=['order']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    def extract_cleaned_sections(self, ten_k_text, sections_df, metadata):\n",
    "        sections = ['item1', 'item1a', 'item1b', 'item2', 'item3', 'item4', 'item5', 'item6', 'item7', \n",
    "                    'item7a', 'item8', 'item9', 'item9a', 'item9b', 'item9c', 'item10', 'item11', 'item12', \n",
    "                    'item13', 'item14', 'item15', 'item16']  \n",
    "        section_titles = {\n",
    "                    'item1': 'Business',\n",
    "                    'item1a': 'Risk Factors',\n",
    "                    'item1b': 'Unresolved Staff Comments',\n",
    "                    'item2': 'Properties',\n",
    "                    'item3': 'Legal Proceedings',\n",
    "                    'item4': 'Mine Safety Disclosures',\n",
    "                    'item5': 'Market for Registrant’s Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities',\n",
    "                    'item6': 'Selected Financial Data',\n",
    "                    'item7': 'Management’s Discussion and Analysis of Financial Condition and Results of Operations',\n",
    "                    'item7a': 'Quantitative and Qualitative Disclosures About Market Risk',\n",
    "                    'item8': 'Financial Statements and Supplementary Data',\n",
    "                    'item9': 'Changes in and Disagreements With Accountants on Accounting and Financial Disclosure',\n",
    "                    'item9a': 'Controls and Procedures',\n",
    "                    'item9b': 'Other Information',\n",
    "                    'item10': 'Directors, Executive Officers and Corporate Governance',\n",
    "                    'item11': 'Executive Compensation',\n",
    "                    'item12': 'Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters',\n",
    "                    'item13': 'Certain Relationships and Related Transactions, and Director Independence',\n",
    "                    'item14': 'Principal Accountant Fees and Services',\n",
    "                    'item15': 'Exhibits and Financial Statement Schedules',\n",
    "                    'item16': 'Form 10-K Summary'  \n",
    "                }\n",
    "        \n",
    "\n",
    "        cleaned_sections = []\n",
    "\n",
    "        for index, section_name in enumerate(sections[:-1]):  # We use -1 because we don't need to process the last item separately\n",
    "            current_start_values = sections_df[sections_df['item'] == section_name]['start'].values\n",
    "            if len(current_start_values) == 0:  # section_name not found in the dataframe\n",
    "                continue\n",
    "            current_start = current_start_values[0]\n",
    "\n",
    "            # Adjusting for the next section's start\n",
    "            if index + 1 < len(sections):\n",
    "                next_start_values = sections_df[sections_df['item'] == sections[index+1]]['start'].values\n",
    "                if len(next_start_values) == 0:  # next section name not found in the dataframe\n",
    "                    next_start = len(ten_k_text)\n",
    "                else:\n",
    "                    next_start = next_start_values[0]\n",
    "            else:\n",
    "                next_start = len(ten_k_text)\n",
    "                \n",
    "            section_title = section_titles.get(section_name, \"\")\n",
    "            section_content = ten_k_text[current_start:next_start] \n",
    "            \n",
    "            current_metadata = metadata.copy()\n",
    "            current_metadata[\"title\"] = section_title\n",
    "        \n",
    "            content = BeautifulSoup(section_content, \"lxml\").get_text(\"\\n\\n\")\n",
    "\n",
    "            cleaned_sections.append({\n",
    "                \"content\": content,\n",
    "                \"metadata\": current_metadata\n",
    "            })\n",
    "\n",
    "        return cleaned_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "56adce9f-0851-4b05-b5fc-b863a54bbf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_splitter = CustomTextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0d9f8f60-1f85-46c1-a34b-f80eca5e24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = custom_splitter.split_text(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0a3e656e-38af-41f9-9251-79b0f40d155d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': 'AGILENT TECHNOLOGIES, INC.',\n",
       " 'date': '20221221',\n",
       " 'sic': 'LABORATORY ANALYTICAL INSTRUMENTS [3826]',\n",
       " 'state': 'CA',\n",
       " 'title': 'Mine Safety Disclosures'}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections[5]['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "fe4e6801-e856-4846-8f3d-957f58fa6c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Instantiate the RecursiveCharacterTextSplitter\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Create an empty list to store the documents\n",
    "documents = []\n",
    "\n",
    "# Iterate over the sections and split large sections into smaller chunks\n",
    "for section in sections:\n",
    "    if len(section['content']) > chunk_size:\n",
    "        chunks = text_splitter.split_text(section['content'])\n",
    "        # Append the section's metadata to each chunk\n",
    "        for chunk in chunks:\n",
    "            modified_chunk = {\n",
    "                \"content\": chunk,\n",
    "                \"metadata\": section[\"metadata\"]\n",
    "            }\n",
    "            # Create a Document object for each chunk and add it to the documents list\n",
    "            document = Document(page_content=modified_chunk['content'], metadata=modified_chunk['metadata'])\n",
    "            documents.append(document)\n",
    "    else:\n",
    "        # Create a Document object for the section and add it to the documents list\n",
    "        document = Document(page_content=section['content'], metadata=section['metadata'])\n",
    "        documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7fa773-fc09-4989-968f-715e44fa1010",
   "metadata": {},
   "source": [
    "## Embedding and Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "fd24e556-e755-4eac-98f3-9d10db009682",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9781d590-858f-489c-bc76-195f699d2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "auth_config = weaviate.AuthApiKey(api_key=\"EVGZZPpXvnby1SqI3sPdquyPFEu10LcC3KbB\")\n",
    "\n",
    "client = weaviate.Client(\n",
    "  url=\"https://streamlit-hackathon-fkg18d0d.weaviate.network\",\n",
    "  auth_client_secret=auth_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "4373adf4-f423-49ed-92b6-a440e200d61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weaviate.client.Client at 0x2887cf8e0>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "671676be-ab44-4599-b36f-bcfa991e6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Weaviate.from_documents(documents, embeddings, client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "1ff8a942-25af-4c5b-b944-62f968050535",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"When was Agilent Technologies incorporated?\"\n",
    "docs = vector_store.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2dcd61fc-4253-4de5-b93c-b1f3eb898eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "9ad570af-a471-4d41-b64d-c48fefa64249",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ac12ded7-c2f0-40ee-bfe6-a8511a385799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'When was Agilent Technologies incorporated?',\n",
       " 'result': 'Agilent Technologies Inc. was incorporated in May 1999.'}"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(chat_model,retriever=vector_store.as_retriever())\n",
    "qa_chain({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22042e1e-2da6-42f0-b151-4c1d31d507e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
